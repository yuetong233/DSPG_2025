[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homepage",
    "section": "",
    "text": "API Call Example\nRemote Sensing Introduction\n[Common Stat Models with R](code/Common_Stat_Models_with_R.html)"
  },
  {
    "objectID": "code/Common_Stat_Models_with_R.html",
    "href": "code/Common_Stat_Models_with_R.html",
    "title": "Common Statistical Models with R Examples",
    "section": "",
    "text": "Simple Linear Regression is a method to model the relationship between one predictor variable (X) and one outcome variable (Y), assuming the relationship is roughly linear. Its purpose is to predict the outcome variable based on the predictor and to quantify how much change in the predictor leads to change in the outcome. The SLR equation takes the form\n\\[\nY= \\beta_0 +\\beta_1 X + \\epsilon,\n\\]\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(\\epsilon\\) is random error. SLR helps us understand and predict how one variable influences another, under assumptions that the relationship is linear, the errors are normally distributed and have constant variance, and observations are independent. In practice, SLR is like drawing the best-fitting straight line through a scatter plot to make sense of patterns in the data. For example, we might ask: how much does a student‚Äôs exam score improve for each extra hour of study?\n\nlibrary(ggplot2)\nlibrary(stats)\n# set your directory\nsetwd(\"~/Desktop/RA,TA,SAIG/DSPG/Stat Lectures\")\n# load the data file\ndata_SLR &lt;- read.csv(file=\"exam_data\")\n# scatter plot\nggplot(data_SLR, aes(x=study_hours, y=exam_score))+\n  geom_point()+\n  labs(title=\"Relationship between Study Hours and Exam Scores\")+\n    theme_minimal()\n\nScatter plots are super important! They help us to see if there is a pattern, a linear trend, or outliers before we fit the model. Here, you see a clear upward trend: more study hours generally means higher exam scores.\nNow, let‚Äôs fit the model: exam_score = \\(\\beta_0\\) + \\(\\beta_1\\) study_hours + \\(\\epsilon_i\\).\n\n# run SLR models with lm function\nSLR &lt;- lm(exam_score~study_hours, data=data_SLR)\n# output\nsummary(SLR)\n\nThe simple linear regression model examined the relationship between exam score and study hours. The intercept of about 37.6 suggests that a student who studied zero hours would be expected to score roughly 37.6 points on the exam, which represents the model‚Äôs baseline. The estimated slope of 5.52 means that for each additional hour of study, the expected exam score increases by about 5.5 points. This effect is highly statistically significant, with a p-value less than 2e-16, providing strong evidence that more study hours are associated with higher exam scores. The model‚Äôs R-squared value is approximately 0.85, indicating that about 85% of the variability in exam scores can be explained by study hours alone, which is quite high and suggests an excellent fit. The residual standard error is about 5.1, meaning the typical difference between the observed scores and the predicted scores is around 5 points. Overall, these results show a strong positive relationship between the number of hours studied and exam performance, with more study time leading to substantially higher predicted scores.\n\n\n\n\n\ndata_SLR$residuals &lt;- resid(SLR)\ndata_SLR$fitted &lt;- fitted(SLR)\nggplot(data_SLR, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(\n    title = \"Homoscedasticity Check\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal()\n\nThis code checks two important assumptions of simple linear regression: independence and constant variance (homoscedasticity) of the residuals. After extracting the residuals and fitted values from the regression model, the code plots the residuals against the fitted values using ggplot2, with a horizontal red reference line at zero.\nThis plot helps to assess whether the residuals are randomly scattered around zero without any clear pattern, which supports the independence assumption (no time or sequence pattern) as well as homoscedasticity, which requires the vertical spread of residuals to remain roughly the same across the range of predicted values.\nIn the displayed plot, the residuals appear fairly randomly distributed around zero, with no obvious systematic pattern or trend, which suggests that the independence assumption is reasonable. Additionally, the spread of the residuals is relatively even across fitted values, supporting the constant variance assumption. There is some slight variability at higher fitted values, but no severe funnel or curved patterns, indicating the model meets these assumptions well enough for valid inference.\n\n\n\n\nggplot(data_SLR, aes(x = study_hours, y = exam_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(\n    title = \"Linearity Check\",\n    x = \"Study Hours\",\n    y = \"Exam Score\"\n  ) +\n  theme_minimal()\n\nThis plot is designed to check the linearity assumption of simple linear regression, which requires that the relationship between the predictor (study hours) and the response variable (exam score) is approximately linear. The code uses ggplot2 to plot the raw data points and overlays a linear trend line with a confidence band using geom_smooth(method = \"lm\"). In the resulting plot, the blue regression line captures the overall trend of the data, while the shaded gray area shows the 95% confidence interval for the line. The pattern of data points closely follows a straight line, with no evidence of curves or major deviations, supporting the linearity assumption. This means a linear model is appropriate for describing the relationship between study hours and exam scores.\nVisualizing the linear relationship in this way is a very important first step before interpreting the model‚Äôs coefficients, because if the true relationship were curved or non-linear, the linear regression results could be misleading. This check reassures us that using a straight line to model these data makes sense.\n\n\n\n\nggplot(data_SLR, aes(x = residuals)) +\n  geom_histogram(bins = 15, fill = \"lightblue\", color = \"black\") +\n  labs(\n    title = \"Histogram of Residuals\",\n    x = \"Residuals\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\nThis plot is used to check the normality assumption of simple linear regression, which says that the residuals should be approximately normally distributed. The code uses ggplot2 to create a histogram of the residuals from the model, with 15 bins and a light blue fill to make the distribution easy to see.\nThe histogram shows the counts of residuals across their range. Ideally, for normality, we hope to see a roughly bell-shaped, symmetric pattern centered around zero. In this case, the histogram appears reasonably symmetric, with most residuals clustered around zero and fewer residuals farther from zero on either side. There are no dramatic gaps or clear skew, although there might be a slightly longer right tail, but nothing extreme.\nThis supports the idea that the residuals are approximately normal, which is important because normal residuals help ensure the reliability of hypothesis tests and confidence intervals in linear regression. It is always good practice to combine this histogram with a QQ plot for a more detailed normality check, which you can do next.\n\nggplot(data_SLR, aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(\n    title = \"Normal Q-Q Plot of Residuals\"\n  ) +\n  theme_minimal()\n\nThis QQ plot is another way to check the normality assumption of residuals in simple linear regression. The code uses ggplot2 to plot the quantiles of the model‚Äôs residuals against the theoretical quantiles from a normal distribution, adding a reference red line with stat_qq_line().\nIf the residuals are approximately normally distributed, then the points on the QQ plot should fall roughly along the red line. In this output, most points follow the red line quite closely in the middle range, with only a few mild deviations at the tails, especially on the upper right side. This is generally acceptable for regression, as slight deviations in the tails are common and not usually a serious concern.\nOverall, this QQ plot supports the histogram finding that the residuals are approximately normal, which validates the regression‚Äôs inference procedures such as confidence intervals and p-values. Using both a histogram and a QQ plot together is a good practice to build confidence that the normality assumption is reasonable for the data."
  },
  {
    "objectID": "code/Common_Stat_Models_with_R.html#model-assumption-check",
    "href": "code/Common_Stat_Models_with_R.html#model-assumption-check",
    "title": "Common Statistical Models with R Examples",
    "section": "",
    "text": "data_SLR$residuals &lt;- resid(SLR)\ndata_SLR$fitted &lt;- fitted(SLR)\nggplot(data_SLR, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(\n    title = \"Homoscedasticity Check\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal()\n\nThis code checks two important assumptions of simple linear regression: independence and constant variance (homoscedasticity) of the residuals. After extracting the residuals and fitted values from the regression model, the code plots the residuals against the fitted values using ggplot2, with a horizontal red reference line at zero.\nThis plot helps to assess whether the residuals are randomly scattered around zero without any clear pattern, which supports the independence assumption (no time or sequence pattern) as well as homoscedasticity, which requires the vertical spread of residuals to remain roughly the same across the range of predicted values.\nIn the displayed plot, the residuals appear fairly randomly distributed around zero, with no obvious systematic pattern or trend, which suggests that the independence assumption is reasonable. Additionally, the spread of the residuals is relatively even across fitted values, supporting the constant variance assumption. There is some slight variability at higher fitted values, but no severe funnel or curved patterns, indicating the model meets these assumptions well enough for valid inference.\n\n\n\n\nggplot(data_SLR, aes(x = study_hours, y = exam_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(\n    title = \"Linearity Check\",\n    x = \"Study Hours\",\n    y = \"Exam Score\"\n  ) +\n  theme_minimal()\n\nThis plot is designed to check the linearity assumption of simple linear regression, which requires that the relationship between the predictor (study hours) and the response variable (exam score) is approximately linear. The code uses ggplot2 to plot the raw data points and overlays a linear trend line with a confidence band using geom_smooth(method = \"lm\"). In the resulting plot, the blue regression line captures the overall trend of the data, while the shaded gray area shows the 95% confidence interval for the line. The pattern of data points closely follows a straight line, with no evidence of curves or major deviations, supporting the linearity assumption. This means a linear model is appropriate for describing the relationship between study hours and exam scores.\nVisualizing the linear relationship in this way is a very important first step before interpreting the model‚Äôs coefficients, because if the true relationship were curved or non-linear, the linear regression results could be misleading. This check reassures us that using a straight line to model these data makes sense.\n\n\n\n\nggplot(data_SLR, aes(x = residuals)) +\n  geom_histogram(bins = 15, fill = \"lightblue\", color = \"black\") +\n  labs(\n    title = \"Histogram of Residuals\",\n    x = \"Residuals\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\nThis plot is used to check the normality assumption of simple linear regression, which says that the residuals should be approximately normally distributed. The code uses ggplot2 to create a histogram of the residuals from the model, with 15 bins and a light blue fill to make the distribution easy to see.\nThe histogram shows the counts of residuals across their range. Ideally, for normality, we hope to see a roughly bell-shaped, symmetric pattern centered around zero. In this case, the histogram appears reasonably symmetric, with most residuals clustered around zero and fewer residuals farther from zero on either side. There are no dramatic gaps or clear skew, although there might be a slightly longer right tail, but nothing extreme.\nThis supports the idea that the residuals are approximately normal, which is important because normal residuals help ensure the reliability of hypothesis tests and confidence intervals in linear regression. It is always good practice to combine this histogram with a QQ plot for a more detailed normality check, which you can do next.\n\nggplot(data_SLR, aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(\n    title = \"Normal Q-Q Plot of Residuals\"\n  ) +\n  theme_minimal()\n\nThis QQ plot is another way to check the normality assumption of residuals in simple linear regression. The code uses ggplot2 to plot the quantiles of the model‚Äôs residuals against the theoretical quantiles from a normal distribution, adding a reference red line with stat_qq_line().\nIf the residuals are approximately normally distributed, then the points on the QQ plot should fall roughly along the red line. In this output, most points follow the red line quite closely in the middle range, with only a few mild deviations at the tails, especially on the upper right side. This is generally acceptable for regression, as slight deviations in the tails are common and not usually a serious concern.\nOverall, this QQ plot supports the histogram finding that the residuals are approximately normal, which validates the regression‚Äôs inference procedures such as confidence intervals and p-values. Using both a histogram and a QQ plot together is a good practice to build confidence that the normality assumption is reasonable for the data."
  },
  {
    "objectID": "code/rs_intro.html",
    "href": "code/rs_intro.html",
    "title": "Introduction to Remote Sensing and Google Earth Engine",
    "section": "",
    "text": "Acknowledgment: I am grateful to Dr.¬†Elinor Benami for the original lecture and lab materials, used here under the Creative Commons License (Attribution-NonCommercial-ShareAlike 4.0 International). Modifications have been made to the context and code of the original content."
  },
  {
    "objectID": "code/rs_intro.html#overview",
    "href": "code/rs_intro.html#overview",
    "title": "Introduction to Remote Sensing and Google Earth Engine",
    "section": "Overview",
    "text": "Overview\nThe purpose of this workshop is to introduce the functionality and structure of Google Earth Engine (GEE) that may support your projects. This tutorial provides a brief overview of the GEE JavaScript interface (Code Editor), imagery data formats, the four types of sensor resolution, time series plots, and how to use RStudio to remotely access GEE resources. For the registration process, please use your personal Google account to access GEE.\n\nLearning Outcomes\n\nNavigate basic Google Earth Engine (GEE) resources\nDescribe the major GEE data types and their associated methods\nCreate and visualize composites based on desired parameters\nObtain climate-related data, land cover information, and vegetation indices\nAccess GEE remotely through RStudio\n\n\n\nSetting up an Account\nTo get started, sign up for Google Earth Engine. Registration is free and straightforward, but it takes approximately 24 hours to be approved to use the code editor.\n\n\nImporting data\nIn addition to providing access to petabytes of satellite imagery and geospatial products, Google Earth Engine (GEE) also enables users to upload and work with their own raster, vector, and tabular data. These user-provided assets can be seamlessly integrated into analyses. This process is automatically linked to the Google Drive account associated with the GEE user account.\nNote that most commonly used export formats in GEE are .tif, .shp, .geojson and .csvfiles\n\n\nGEE processes on Google Cloud Platform\nWhen you open the Code Editor in your browser, you can write and run JavaScript code locally. To access GEE‚Äôs cloud computing resources, you can use server-side functions. For example, the ee prefix before the ImageCollection tells Earth Engine to process the data on its servers. Without that indicator, GEE will cede operations to the server. The satellite image collections listed below include sensors that capture climate variables, surface reflectance data, and processed (derived) products such as land cover classifications.\nIf we want to access climate-related data, including temperature and precipitation, we can use the MODIS Land Surface Temperature (LST) and write the following code:\nvar modisLST = ee.ImageCollection('MODIS/061/MOD11A1');\nTo compute vegetation or surface indices using land surface reflectance data, we can use the Landsat8 Surface Reflectance data and implement the following code:\nvar landsat8Sr = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2');\nTo obtain a Land Use/Land Cover (LULC) dataset with class probabilities and label information for nine land cover types, we can use the Dynamic World V1 dataset and write the following code:\nvar dynamicv1 = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1');\n\n\nJavaScript\nSeveral resources can help you in working with JavaScript.\n\nJavascript.info provides a comprehensive guide; focus on Part I for this tutorial.\nW3Schools offers clear explanations of individual JavaScript components.\nJavaScript & JQuery is a well-designed book with fundamentals, illustrations, and practical examples."
  },
  {
    "objectID": "code/rs_intro.html#images-and-image-collections",
    "href": "code/rs_intro.html#images-and-image-collections",
    "title": "Introduction to Remote Sensing and Google Earth Engine",
    "section": "2.1 Images and Image Collections",
    "text": "2.1 Images and Image Collections\n\nImages\nImages are Raster objects composed of one or more bands.\n\nEach band is essentially a layer of data with a unique:\n\nName: A unique identifier for the band\nData type: Specifies the format (e.g., integer, float)\nScale: The spatial resolution (e.g., meters per pixel)\nMask: Indicates which pixels are valid or missing\nProjection: Defines how the image is georeferenced\n\nMetadata, stored as a set of properties for that band.\n\nYou can create images from constants (e.g., a fixed value across space), lists, or other objects. In the code editor ‚ÄúDocs‚Äù tab, you‚Äôll find numerous processes that can be applied to images (e.g.¬†masking, scaling, or visualizing).\nDo not confuse an individual image with an image collection. An Image refers to a single raster dataset, whereas an ImageCollection is a set of images grouped together, most often as a time series, and often known as a stack.\n\n\nImage Collections\nThe code below is an established method of extracting one individual image from an image collection. You can copy and paste this code into the GEE code editor.\nOn the first line, we see that we are creating a JavaScript variable named collection and assign it a Landsat 8 surface reflectance image collection. Then we use ee before ImageCollection to tell GEE to access data from its cloud-based servers. The dataset ‚ÄòLANDSAT/LC08/C02/T1_L2‚Äô is the Landsat 8 Collection 2 Tier 1 Surface Reflectance product, which provides atmospherically corrected reflectance data from the OLI and TIRS sensors.\nThe following steps further refine the extraction of an image from an image collection.\n\n.select selects the specific band from each image in the collection.\n.filterBounds filters data to the area specified, in this case a geometry Point that was created within GEE.\n.filterDate narrows the collection to images captured within a specific date range.\n.first is a JavaScript method of choosing the first image from a (potentially sorted) list. Often used after sorting by cloud cover to get the clearest image.\n\nMap.centerObject() centers the map on the image, and the number is the amount of zoom. The higher that value is, the more zoomed in the image is - this often requires some trial-and-error to find a good fit.\nMap.addLayer() adds the visualization layer to the map. Each image or image collection has unique band names, so always refer to the documentation for the correct ones. GEE uses Red-Green-Blue band order for true-color visualization. min and max are the values that normalize the value of each pixel to the conventional 0-255 color scale. In this case, although the maximum value of a pixel in all three of those bands is 2000, for visualization purposes GEE will normalize that to 255, the max value in a standard 8-bit image.\nThere is a comprehensive guide on imagery visualization covers topics like false-color composites, mosaicking, and single-band display.\n// Load the Landsat 8 Tier 1 L2 SR\nvar collection = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n  .filterBounds(ee.Geometry.Point(-80.4139, 37.2296))\n  .filterDate('2024-01-01', '2024-01-31')\n  .sort('CLOUD_COVER');  // lowest cloud cover first\n  \nvar first = collection.first(); // Select the first (least‚Äêcloudy) image\nMap.centerObject(first, 8); // Center the map on that image\n\n// Display the true-color SR bands\nMap.addLayer(first, {bands: ['SR_B4', 'SR_B3', 'SR_B2'], min: 0, max: 3000}, 'first');\n\n\nSensed versus Derived Imagery\nGEE provides a rich suite of dataset, and while many of them are traditional sensed imagery, others are derived dataset. For instance, the Dynamic World dataset is generated from Sentinel-2 Level 1C imagery using machine learning. If you explore its ‚ÄúBand‚Äù information, you‚Äôll find refined land cover classes representing specific surface types. Datasets like this often require different visualization techniques or approaches for mosaicking compared to raw imagery.\nThe code below uses the Dynamic World dataset to visualize land cover centered at Blacksburg from June to August 2024. Cropland (Class 4) is highlighted in green, and a full land cover layer displaying all nine classes is also included.\n// Load the Dynamic World Image\nvar point = ee.Geometry.Point(-80.4139, 37.2296);\nvar dwCollection = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1')\n                     .filterBounds(point) // Filter to the Blacksburg\n                     .filterDate('2024-06-01', '2024-08-31');\n\n// Check if the collection contains any images for the period and location\nvar collectionSize = dwCollection.size();\n\n// If images are found, create a composite and classify\nif (collectionSize.gt(0)) { // Check if there's at least one image\n  // Dynamic World provides a 'label' band with the top classification.\n  \n  var dwImage = dwCollection.select('label').mode(); // Get the most frequent class\n\n  // Dynamic World classification: class 4 = Crops\n  var agriculture = dwImage.eq(4); // Binary image: 1 = cropland, 0 = other\n\n  // Define visualization parameters\n  var visParams = {\n    min: 0,\n    max: 1,\n    palette: ['white', 'red'] // non-agriculture = white, agriculture = green\n  };\n\n  // Center the map on the point of interest with an appropriate zoom level\n  Map.centerObject(point, 10); // Zoom level 10 is a suggestion, adjust as needed\n  Map.addLayer(agriculture.selfMask(), visParams, 'Dynamic World Cropland (Class 4)'); \n  // Map.addLayer(agriculture, visParams, 'Dynamic World Cropland (Class 4)'); \n  // Show both agriculture and non-agriculture lands\n\n  // Add the Dynamic World composite label layer to see all classes\n  var dwVisParams = {\n    min: 0,\n    max: 8, // Dynamic World has 9 classes (0-8)\n    palette: [\n      '#419BDF', // Water\n      '#397D49', // Trees\n      '#88B053', // Grass\n      '#7A87C6', // Flooded vegetation\n      '#E49635', // Crops\n      '#DFC35A', // Shrub and scrub\n      '#C4281B', // Built area\n      '#A59B8F', // Bare ground\n      '#B39FE1'  // Snow and ice\n    ]\n  };\n\n  Map.addLayer(dwImage, dwVisParams, 'Dynamic World Labels (Mode)', false); // Add as a hidden layer initially // The most likely reason no color appears in the image is cloud cover during the selected date range.\n\n  // Center the map on the point of interest even if no data is found\n  Map.centerObject(point, 9);\n}\n\n// Add the point of interest to the map to verify location\nMap.addLayer(point, {color: 'green'}, 'Point of Interest');"
  },
  {
    "objectID": "code/rs_intro.html#geometries",
    "href": "code/rs_intro.html#geometries",
    "title": "Introduction to Remote Sensing and Google Earth Engine",
    "section": "2.2 Geometries",
    "text": "2.2 Geometries\nGoogle Earth Engine (GEE) handles vector data using the Geometry type. While it generally follows the traditional forms of vector data‚ÄîPoints, Lines, and Polygons‚ÄîGEE includes some additional structure and terminology:\n\nPoint: A single location defined by latitude and longitude.\nLineString: A connected list of points that does not start and end at the same location; represents a linear path.\nLinearRing: A closed version of a LineString that starts and ends at the same location; used to define polygon boundaries.\nPolygon: A list of LinearRings. The first LinearRing defines the outer boundary (shell), and any additional LinearRings define interior holes (e.g., lakes or islands within the polygon).\n\nGEE also recognizes MultiPoint, MultiLineString and MultiPolygon, which are simply collections of more than one element. Additionally, you can combine any of these together to form a MultiGeometry.\nOnce you have a set of geometries, there are geospatial operations you can use for analysis, such as building buffer zones, area analysis, converting geometries to raster format (rasterization), etc. The documentation contains some basic examples to show you how to get started, although there are many more functions listed under the ‚ÄòDocs‚Äô tab in the Code Editor.\nThe following code demonstrates how to define a 30 km radius around a point in Blacksburg and compare urban area changes over time using imagery from both LandSat 5 and LandSat 8. Landsat 5 provides historical data from 1984 to 2012, while Landsat 8 covers the period from 2013 to the present. To ensure a meaningful visual comparison, specific bands were selected from each satellite that are compatible in terms of wavelength and resolution.\n// Setup: Blacksburg coordinates and Region of Interest (ROI)\nvar blacksburgPoint = ee.Geometry.Point([-80.4139, 37.2296]); // Longitude, Latitude\nvar roi = blacksburgPoint.buffer(30000); // 30km radius buffer\n\n// Center the map on the ROI\nMap.centerObject(roi, 9); // Zoom level 9 is a good starting point for 50km radius\n\n// Define Time Periods\nvar earlyPeriod = {\n  start: '2000-01-01',\n  end: '2004-12-31',\n  label: '2000-2004'\n};\n\nvar recentPeriod = {\n  start: '2019-01-01',\n  end: '2023-12-31',\n  label: '2019-2023'\n};\n\n// Image Preparation Function (Scaling and Cloud Masking for Landsat Collection 2 Level 2)\nfunction prepLandsatC2L2(image) {\n  // Apply scaling factors to optical bands.\n  // See: https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C02_T1_L2#bands\n  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  // Note: Thermal bands (ST_B*) would need different scaling if used.\n\n  // Cloud and shadow masking using QA_PIXEL band\n  // Bits (binary digits): 1 (Dilated Cloud), 3 (Cloud), 4 (Cloud Shadow), often used to encode quality flags for each pixel\n  // For example, if a pixel's QA value is 20, its binary representation is 10100.\n // Reading from right to left (bit 0 to bit 4):\n// Bit 0: (unused), Bit 1: Dilated Cloud = 0, Bit 2: (unused), Bit 3: Cloud = 1, Bit 4: Cloud Shadow = 1\n// This means the pixel is flagged for both cloud and cloud shadow.\n  var qa = image.select('QA_PIXEL');\n  var dilatedCloudBit = 1 &lt;&lt; 1;\n  var cloudBit = 1 &lt;&lt; 3;\n  var cloudShadowBit = 1 &lt;&lt; 4;\n\n  // Mask should be 0 for clear conditions.\n  var mask = qa.bitwiseAnd(dilatedCloudBit).eq(0)\n      .and(qa.bitwiseAnd(cloudBit).eq(0))\n      .and(qa.bitwiseAnd(cloudShadowBit).eq(0));\n\n  // Add scaled optical bands back to the image and apply the mask.\n  // Select only the scaled optical bands for the composite.\n  return image.addBands(opticalBands, null, true)\n              .updateMask(mask)\n              .select(['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7'],  // L5 bands\n                      ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7']); // L8 bands\n}\n\n\n// Load and Process Early Period Imagery (Landsat 5)\nvar l5Collection = ee.ImageCollection('LANDSAT/LT05/C02/T1_L2')\n                      .filterBounds(roi)\n                      .filterDate(earlyPeriod.start, earlyPeriod.end)\n                      .map(prepLandsatC2L2);\n\nvar earlyComposite = l5Collection.median().clip(roi);\n\n// Load and Process Recent Period Imagery (Landsat 8)\n// Can also use LANDSAT/LC09/C02/T1_L2 or merge L8 and L9 for more data.\nvar l8Collection = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\n                      .filterBounds(roi)\n                      .filterDate(recentPeriod.start, recentPeriod.end)\n                      .map(prepLandsatC2L2);\n\nvar recentComposite = l8Collection.median().clip(roi);\n\n// Define Visualization Parameters for True Color\n// Reflectance values typically range from 0 to 1 after scaling.\n// We display values between 0.0 and 0.35 (or 0.4) for good contrast.\nvar trueColorVisParams = {\n  min: 0.0,\n  max: 0.35, // Adjusted for better urban brightness\n};\n\nvar l5TrueColorVis = {\n  bands: ['SR_B3', 'SR_B2', 'SR_B1'], // Red, Green, Blue for Landsat 5\n  min: trueColorVisParams.min,\n  max: trueColorVisParams.max\n};\n\nvar l8TrueColorVis = {\n  bands: ['SR_B4', 'SR_B3', 'SR_B2'], // Red, Green, Blue for Landsat 8\n  min: trueColorVisParams.min,\n  max: trueColorVisParams.max\n};\n\n// Add Layers to Map\nMap.addLayer(earlyComposite, l5TrueColorVis, 'True Color ' + earlyPeriod.label + ' (L5)');\nMap.addLayer(recentComposite, l8TrueColorVis, 'True Color ' + recentPeriod.label + ' (L8)');\n\n// Add ROI outline to the map\nvar empty = ee.Image().byte(); // Create an empty image\nvar roiOutline = empty.paint({\n  featureCollection: roi,\n  color: 1, // arbitrary color, won't be shown\n  width: 2  // outline width\n});\nMap.addLayer(roiOutline, {palette: 'FF0000'}, '50km Radius ROI'); // Red outline\n\n// Print information to the console (optional)\nprint('Early Period Composite (Landsat 5):', earlyComposite);\nprint('Recent Period Composite (Landsat 8):', recentComposite);\nprint('Region of Interest (50km radius around Blacksburg):', roi);"
  },
  {
    "objectID": "code/rs_intro.html#features-and-boundries",
    "href": "code/rs_intro.html#features-and-boundries",
    "title": "Introduction to Remote Sensing and Google Earth Engine",
    "section": "2.3 Features and Boundries",
    "text": "2.3 Features and Boundries\n\nFeatures\nAt the most basic definition, a Feature in GEE is an object which stores a geometry property (Point, Line, Polygon) along with it‚Äôs associated properties. GEE uses the GeoJSON format to store and transmit these features.\nSuppose we create an individual point that we want to associate with collected data. Based on the defined point variable, we can generate a surrounding square geometry and then create a feature using that geometry. The feature is assigned attribute values‚Äîin this case, latitude, longitude, and agricultural status. The resulting variable, agriculture, stores both the spatial information and the descriptive data for that location.\n// Define the point and square polygon centered at Blacksburg\nvar point = ee.Geometry.Point(-80.4139, 37.2296);\nvar squareSize = 5000; // meters\nvar square = point.buffer(squareSize).bounds();\nMap.addLayer(square, {color: 'red'}, 'Analysis Square');\nMap.centerObject(square, 13);\n\n// Load Dynamic World and compute mode composite\nvar startDate = '2024-06-01';\nvar endDate = '2024-08-31';\n\nvar dwCollection = ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1')\n  .filterBounds(square)\n  .filterDate(startDate, endDate);\n\nvar dwImage = dwCollection.select('label').mode();\nvar agriculture = dwImage.eq(4); // 1 = Cropland\n\nMap.addLayer(agriculture.selfMask(), {palette: ['green']}, 'Cropland Mask');\n\n// Reduce to histogram\nvar cropStats = agriculture.reduceRegion({\n  reducer: ee.Reducer.frequencyHistogram(),\n  geometry: square,\n  scale: 10,\n  maxPixels: 1e8\n});\n\n// Evaluate and create a Feature\ncropStats.evaluate(function(result) {\n  var histogram = result.label || {};\n  var cropPixels = histogram['1'] || 0;\n  var nonCropPixels = histogram['0'] || 0;\n  var total = cropPixels + nonCropPixels;\n  var cropPercent = total &gt; 0 ? (cropPixels / total) * 100 : 0;\n\n  // Print results\n  print('Cropland Pixels:', cropPixels);\n  print('Cropland Coverage (%):', cropPercent.toFixed(2));\n\n  // Create an ee.Feature with square geometry and attributes\n  var labeledFeature = ee.Feature(square, {\n    site_name: 'Point of Interest Alpha',\n    latitude: point.coordinates().get(1),\n    longitude: point.coordinates().get(0),\n    start_date: startDate,\n    end_date: endDate,\n    crop_pixels: cropPixels,\n    non_crop_pixels: nonCropPixels,\n    total_pixels: total,\n    cropland_percent: cropPercent,\n    data_source: 'GOOGLE/DYNAMICWORLD/V1'\n  });\n\n  // Add it to a FeatureCollection so it can be exported or mapped\n  var fc = ee.FeatureCollection([labeledFeature]);\n  print('üßæ Feature with cropland coverage:', fc);\n\n  // Map visualization (colored point or polygon)\n  Map.addLayer(labeledFeature, {color: 'orange'}, 'Labeled Analysis Feature');\n\n  // Export to Google Drive as CSV\n  Export.table.toDrive({\n    collection: fc,\n    description: 'Cropland_Percent_Square_Polygon',\n    fileFormat: 'CSV'\n  });\n});\nOnce you have information in a Feature, you can filter it to find specific information, such as the name of an object or based on the size of a polygon, or provide aggregated analysis. The documentation on working with Feature Collections is comprehensive, and provides many ideas on how to use them efficiently in your analysis.\n\n\nBoundaries\nIn Google Earth Engine, a boundary refers to the area represented by a polygon‚Äîcommonly used to define the spatial extent for analyzing satellite or other geospatial data. Boundaries in GEE are stored as polygon geometries within ee.Feature or ee.FeatureCollection objects and can include additional attribute information.\nYou can either\n\nLoad your own shapefiles or GeoJSONs as assets\nImport administrative boundaries, such as countries or counties, from GEE-hosted datasets\n\nThe following code loads the TIGER/Line state boundaries and specifically filters for the state of Virginia.\n// Load TIGER/Line state boundaries from GEE\nvar states = ee.FeatureCollection('TIGER/2018/States'); \nvar virginia = states.filter(ee.Filter.eq('NAME', 'Virginia')); // Filter to Virginia using its state FIPS code (51) or name\n\nMap.centerObject(virginia, 7); // Set the zoom level to 7\nMap.addLayer(virginia, {color: 'black'}, 'Virginia Boundary'); // Center the map and add the boundary\n\nCombine the Pixels within A Certain Boundary\nCombining or summarizing pixels within a specific boundary in GEE is a common task for extracting zonal statistics (e.g., mean NDVI within a county). This involves reducing the pixel-level data by region.\nKey functions include:\n\nee.Reducer.mean() calculates the mean of pixel values.\ngeometry or featureCollection defines the boundary/region.\nreduceRegion() or reduceRegions() to reduce data over one or multiple geometries.\n\nThe following code show the case to use MODIS (Terra Land Surface Temperature and Emissivity Daily Global 1km) to compute the weekly averaged Land Surface Temperature (LST) for the entire Virginia from January to March 2024.\nWe first collect temperature data from each pixel within the desired time period and overlay it with the state boundary. For each week, a mean LST image is computed, converted to Celsius, and multiplied by a fractional mask to isolate pixel values within the boundary. Pixels with at least 50% of their area inside the boundary are included in the analysis.The area-weighted mean is then calculated by dividing the sum of weighted LST values by the sum of the mask values.\n// Load Virginia boundary\nvar states = ee.FeatureCollection('TIGER/2018/States');\nvar virginiaFc = states.filter(ee.Filter.eq('NAME', 'Virginia'));\nvar virginia = virginiaFc.geometry();\n\n// Set time range\nvar startDate = ee.Date('2024-01-01');\nvar endDate = ee.Date('2024-03-31');\n\n// Load MODIS Terra LST (daytime band)\nvar modis = ee.ImageCollection('MODIS/061/MOD11A1')\n  .select('LST_Day_1km')\n  .filterDate(startDate, endDate)\n  .filterBounds(virginia);\n\n// Define spatial scales\nvar modisScale = 1000;  // MODIS resolution\nvar fineScale = 100;    // High-resolution mask scale\n\n// Create high-resolution binary mask of Virginia\nvar highResMask = ee.Image.constant(1)\n  .clip(virginia)\n  .reproject({\n    crs: 'EPSG:4326',\n    scale: fineScale\n  });\n\n// Compute fractional overlap at MODIS scale\nvar fracMask = highResMask\n  .reduceResolution({\n    reducer: ee.Reducer.mean(),\n    maxPixels: 1024\n  })\n  .reproject({\n    crs: 'EPSG:4326',\n    scale: modisScale\n  });\n\n// Apply &gt;= 50% inclusion threshold\nvar binaryMask = fracMask.gte(0.5);  // 1 where &gt;=50% of the pixel is inside the boundary; 0 elsewhere\n\n// Determine weekly intervals\nvar nWeeks = ee.Number(endDate.difference(startDate, 'week')).ceil();\nvar weekList = ee.List.sequence(0, nWeeks.subtract(1));\n\n// Loop over weeks and compute masked mean LST\nvar weeklyStats = ee.FeatureCollection(\n  weekList.map(function(i) {\n    i = ee.Number(i);\n    var wkStart = startDate.advance(i, 'week');\n    var wkEnd = wkStart.advance(1, 'week');\n\n    // Compute weekly average LST image in Celsius\n    var wkMeanImage = modis\n      .filterDate(wkStart, wkEnd)\n      .mean()\n      .multiply(0.02)\n      .subtract(273.15)\n      .rename('LST_mean_C');\n\n    // Apply binary mask using updateMask (removes pixels outside inclusion zone)\n    var maskedLST = wkMeanImage.updateMask(binaryMask);\n\n    // Compute mean LST only for included pixels\n    var meanLST = maskedLST.reduceRegion({\n      reducer: ee.Reducer.mean(),\n      geometry: virginia,\n      scale: modisScale,\n      maxPixels: 1e13\n    });\n\n    return ee.Feature(null, {\n      'week_start': wkStart.format('YYYY-MM-dd'),\n      'week_end': wkEnd.format('YYYY-MM-dd'),\n      'LST_mean_C': meanLST.get('LST_mean_C')\n    });\n  })\n);\n\n// Print results\nprint('Weekly weighted mean LST (¬∞C) for Virginia, Jan‚ÄìMar 2024', weeklyStats);\n\n// Export to Drive\nExport.table.toDrive({\n  collection: weeklyStats,\n  description: 'Virginia_Weekly_LST_Thresholded',\n  fileNamePrefix: 'Virginia_Weekly_LST_50pctMask_JanMar2024',\n  fileFormat: 'CSV'\n});"
  },
  {
    "objectID": "code/rs_intro.html#arrays",
    "href": "code/rs_intro.html#arrays",
    "title": "Introduction to Remote Sensing and Google Earth Engine",
    "section": "2.4 Arrays",
    "text": "2.4 Arrays\nArrays are structured collections of data elements stored in a defined order, often in contiguous memory. A one-dimensional array is a simple list of values, while multi-dimensional arrays organize data in grids or higher-order structures. A two-dimensional array is commonly known as a matrix, consisting of rows and columns. While terminology may vary across disciplines, understanding arrays is key to working with data structures in GEE.\nGoogle Earth Engine is not optimized for general array-based math, and using arrays outside of its built-in functions can lead to poor performance. While there‚Äôs a helpful video on the engineering behind GEE, this workshop will only cover basic array transformations, such as aggregation and filtering. For complex array operations, it‚Äôs often better to export the data and use a specialized framework."
  },
  {
    "objectID": "code/rs_intro.html#methods-reducers-and-joins",
    "href": "code/rs_intro.html#methods-reducers-and-joins",
    "title": "Introduction to Remote Sensing and Google Earth Engine",
    "section": "2.5 Methods: Reducers and Joins",
    "text": "2.5 Methods: Reducers and Joins\n\nReducers\nSo far, we‚Äôve worked with objects like Images, Features, and Geometries. Reducers are a method of aggregating data for analysis. For instance, we could take an Image Collection and use reducer to find the average value at each pixel, resulting in a single layer. Or we could reduce an image to a set of regions, grouping similar data together to create a simplified map. Reducers can be applied to both Images and Features, with specific functions for each. They can also be combined to create analysis chains.\nIn the example below, the variable collection filters imagery for the year 2022 over a point in Blacksburg. The variable maxLST then uses a reducer to extract the maximum land surface temperature in Kelvin for that location.\n// Load and filter the MODIS image collection.\nvar collection = ee.ImageCollection('MODIS/006/MOD11A2')\n  .select('LST_Day_1km')\n  .filterDate('2022-01-01', '2022-12-31');\n\nvar scaleAndMaskLST = function(image) {\n  var scaledLST = image.multiply(0.02); // MODIS LST values are stored as scaled integers; multiply them by 0.02 to convert to Kelvin.\n  var maskedLST = scaledLST.updateMask(\n    image.neq(0).and(image.gte(7500)).and(image.lte(65535)) // Exclude pixels with a value of 0, keep only pixels with scaled LST greater than 7500, and exclude values outside the valid MODIS range.\n  ); \n  return maskedLST;\n};\n\nvar processedCollection = collection.map(scaleAndMaskLST);\n\n// Define a point for Blacksburg, VA (using coordinates from search results)\nvar blacksburg = ee.Geometry.Point(-80.4139, 37.2296);\n\n// Reduce the collection to a single image representing the maximum LST\nvar maxLSTImage = processedCollection.reduce(ee.Reducer.max());\n\n// Get the maximum LST value at the Blacksburg point\nvar maxLSTKelvin = maxLSTImage.reduceRegion({\n  reducer: ee.Reducer.max(),\n  geometry: blacksburg,\n  scale: 1000 // Use the MODIS resolution (1km)\n}).get('LST_Day_1km_max');\n\n// Convert Kelvin to Fahrenheit: F = (K - 273.15) * 9/5 + 32\nvar maxLSTFahrenheit = ee.Number(maxLSTKelvin)\n  .subtract(273.15)\n  .multiply(9)\n  .divide(5)\n  .add(32);\n\n// Print the results\nprint('Maximum LST in Blacksburg (Kelvin):', maxLSTKelvin);\nprint('Maximum LST in Blacksburg (Fahrenheit):', maxLSTFahrenheit);\nThere are hundreds of different operations for using Reducer, with the functions listed in the ‚ÄòDocs‚Äô tab. Certain functions will only work with specific object types, but follow along with the Reducer will help you understand how to aggregate data and extract useful results. Getting familiar with Reducer is an essential component to working with GEE.\n\n\nJoin\nThis process links information from one dataset to another based on a shared attribute. For example, you can join a Landsat ImageCollection from early 2016 with a FeatureCollection of agriculture locations from the past decade, filtered to the same area. The Join keeps only relevant data, combining useful information from both sources. While there are different types of joins, the key idea is merging data meaningfully.\nAlthough there are different types of joins, the process brings information together, keeping only relevant information. The documentation on Joins goes over specific examples and concepts, and it‚Äôs important to understand which type of join best fits your analysis. The three most common join types in GEE are:\n\nLeft Join: Retains all records from the primary dataset and adds matching information from the secondary dataset when available.\nInner Join: Keeps only the records where both the primary and secondary datasets have a matching entry.\nSpatial Join: Combines datasets based on geographic location (ie, keeping only the features from one dataset that fall within a specified buffer)"
  },
  {
    "objectID": "code/rs_intro.html#four-types-of-resolution",
    "href": "code/rs_intro.html#four-types-of-resolution",
    "title": "Introduction to Remote Sensing and Google Earth Engine",
    "section": "Four Types of Resolution",
    "text": "Four Types of Resolution\n\nSpatial resolution: The size of the smallest object that can be detected (i.e., pixel size). \nTemporal resolution: The frequency at which a sensor revisits the same location on Earth (i.e., revisit time or orbital period).\nSpectral resolution: The ability of a sensor to distinguish between different wavelengths (i.e., number and width of spectral bands). \nRadiometric resolution: The sensor‚Äôs sensitivity to differences in signal intensity or reflectance (i.e., bit depth). \n\nSuppose we are working with Landsat 8 and want to find key details such as the image scale, the number of images within a specific time range, the number of bands, and the data type of each band (across four resolutions). We can either look up this information in the documentation or use the following code to extract it directly.\n// Load a Landsat 8 image collection.\nvar collection = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2');\n\n// It's a good practice to filter by date or location for a specific image.\nvar image = collection.first(); \n// Here, I manually select the first image in the collection based on the internal ordering, \n// which is typically the earliest image from the satellite (chronological).\n\n// --- Spatial Resolution ---\n// The nominal scale (in meters) of the image's default projection.\n// For Landsat 8, most bands are 30 meters, the panchromatic band is 15 meters.\nvar spatialResolution = image.projection().nominalScale();\nprint('Spatial Resolution (nominal scale in meters):', spatialResolution);\n\n// You can also get the resolution of specific bands.\n// For example, band B4 (Red)\nvar bandB4 = image.select('SR_B4');\nvar spatialResolutionB4 = bandB4.projection().nominalScale();\nprint('Spatial Resolution for SR_B4 (Red band) in meters:', spatialResolutionB4);\n\n// --- Temporal Resolution ---\n// Landsat 8 has a nominal temporal resolution (revisit period) of 16 days.\n// This is a characteristic of the satellite's orbit, not a property directly\n// extractable from a single image object in GEE.\n// However, you utilize this resolution when filtering image collections by date.\nvar temporalResolutionComment = 'Temporal Resolution: Landsat 8 has a nominal revisit period of 16 days.';\nprint(temporalResolutionComment);\n\n// Example of how to use temporal resolution:\n// Filtering the collection to see how many images are available within a 30-day window\nvar startDate = ee.Date('2023-01-01');\nvar endDate = startDate.advance(30, 'day');\nvar imagesIn30Days = collection.filterDate(startDate, endDate);\nprint('Number of images available in a 30-day window (illustrates temporal sampling):', imagesIn30Days.size());\n\n// --- Spectral Resolution ---\n// This refers to the number and width of the spectral bands.\n// We can list the band names.\nvar bandNames = image.bandNames();\nprint('Spectral Bands (Names):', bandNames);\n\n// To get more detailed spectral information (e.g., wavelength ranges, data types),\n// inspect the 'bandTypes()' object. This object contains properties for each band,\n// which may include 'wavelength_min', 'wavelength_max', 'data_type', etc.,\n// if they are part of the dataset's metadata in GEE.\nprint('All image band properties (inspect this object for spectral details):', image.bandTypes());\n\n// --- Radiometric Resolution ---\n// This is typically indicated by the data type (e.g., int8, int16, float32).\n// 'int16' means 16-bit radiometric resolution.\n// Access the data_type property from the bandTypes() dictionary for SR_B4.\nvar bandTypesDict = image.bandTypes();\nvar srb4Type = bandTypesDict.get('SR_B4');\nprint('Radiometric Resolution (Data Type for SR_B4):', srb4Type);\nprint('Image Data Type (dictionary of all band types):', bandTypesDict);\n\n// You can also check the data type of the entire image (which is a dictionary of all band types)\nprint('Image Data Type (overall, as a dictionary of band types):', image.bandTypes());\n\n\n// Add the image to the map for visual inspection\nMap.centerObject(image, 10);\nMap.addLayer(image, {bands: ['SR_B4', 'SR_B3', 'SR_B2'], min: 0, max: 20000}, 'Landsat 8 True Color');\nEach pixel has a geographic coordinate system, defined relative to the axes of a coordinate reference system (CRS). In Earth Engine, the CRS is often referred to as a projection because it includes the model of the Earth‚Äôs shape (datum) and the mathematical transformation that maps a 3D shape onto a 2D surface. This projection determines how spatial data is represented on a flat map."
  },
  {
    "objectID": "code/rs_intro.html#additional-resources",
    "href": "code/rs_intro.html#additional-resources",
    "title": "Introduction to Remote Sensing and Google Earth Engine",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGoogle Earth Engine link\nCode Editor Map ‚Äì what all the features on the code editor mean\nGoogle Earth Engine Blog\nVideo tutorials on using GEE (from the Earth Engine Users‚Äô Summit)"
  },
  {
    "objectID": "code/api_call.html",
    "href": "code/api_call.html",
    "title": "Use of BLS API",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "code/api_call.html#setup",
    "href": "code/api_call.html#setup",
    "title": "Use of BLS API",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "code/api_call.html#use-the-bls-api-to-retrieve-the-data",
    "href": "code/api_call.html#use-the-bls-api-to-retrieve-the-data",
    "title": "Use of BLS API",
    "section": "Use the BLS API to retrieve the data",
    "text": "Use the BLS API to retrieve the data\n\n# Set the library and install the packages\nlibrary(devtools)\nlibrary(blsR)\n\n# The introduction about this package: https://cran.r-project.org/web/packages/blsR/blsR.pdf\ndevtools::install_github(\"groditi/blsR\")\n\n# Acquire the API by registering through https://www.bls.gov/developers/home.htm\nbls_set_key(\"your personal API\")\n\ntest_series &lt;- get_series(series_id = \"series id you found\", \n                         start_year = 2016, \n                         end_year = 2024, \n                         api_key = bls_get_key())\n\nI work through the series number by https://data.bls.gov/PDQWeb/la, and we discover that the series number for each county is ‚ÄúFCN+Fips code+00000000‚Äù"
  },
  {
    "objectID": "code/api_call.html#acs-api",
    "href": "code/api_call.html#acs-api",
    "title": "Use of BLS API",
    "section": "ACS API",
    "text": "ACS API\n\n# Set the library and install the packages\nlibrary(censusapi)\nlibrary(tidycensus) \n\n# Acquire the API by registering through https://api.census.gov/data/key_signup.html\n\ncensus_api_key(\"your api key\", install = TRUE, overwrite = TRUE)\n# Acquire the API by registering through https://api.census.gov/data/key_signup.html\n\n#The main function to retrive the data here are https://cran.r-project.org/web/packages/tidycensus/tidycensus.pdf\n\n#to check the name of the variable\nv15 &lt;- load_variables(2015, \"acs5\", cache = TRUE)\nView(v15)\n\n# for example, if I want to know the age\n# here is the explanation of the codebook: https://data.census.gov/table/ACSDT5Y2022.B01001\n\nnc_acs_2015 &lt;- get_acs(geography = \"county\", \n              year = 2015,\n              variables = c(age = \"B01001A_003\"), \n              state = \"NC\",\n              survey = \"acs5\",\n              output = \"wide\")"
  }
]